{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About this kernel\n",
    "\n",
    "The goal is to:\n",
    "* Investigate the problem of null masks by exploring the distribution of NaN per image.\n",
    "* Reduce image size to 224x224 for simpler models.\n",
    "* Create a lightweight CNN to predict if a certain image has no defect (i.e., it has 4 missing masks). This will be useful in order to reduce the computation power needed to train a segmentation model (e.g. Mask R-CNN), since we can immediately discard the image with 4 missing masks.\n",
    "\n",
    "This is a work in progress, and I will update the kernel in the next few days. I'll work hard on this if you show some support :)\n",
    "\n",
    "### Updates\n",
    "* V9: Changed model from MobileNet to DenseNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.applications import DenseNet121\n",
    "from keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, Nadam\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0,1,2,3'   #指定第一块GPU可用  \"0,1\"\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.7  # 程序最多只能占用指定gpu50%的显存\n",
    "config.gpu_options.allow_growth = True      #程序按需申请内存\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50272, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId_ClassId</th>\n",
       "      <th>EncodedPixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0002cc93b.jpg_1</td>\n",
       "      <td>29102 12 29346 24 29602 24 29858 24 30114 24 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002cc93b.jpg_2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0002cc93b.jpg_3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0002cc93b.jpg_4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00031f466.jpg_1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ImageId_ClassId                                      EncodedPixels\n",
       "0  0002cc93b.jpg_1  29102 12 29346 24 29602 24 29858 24 30114 24 3...\n",
       "1  0002cc93b.jpg_2                                                NaN\n",
       "2  0002cc93b.jpg_3                                                NaN\n",
       "3  0002cc93b.jpg_4                                                NaN\n",
       "4  00031f466.jpg_1                                                NaN"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('../input/train.csv')\n",
    "\n",
    "print(train_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('JPEG', (1600, 256))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看图片大小\n",
    "path = os.path.join(\"../input/train_images\", \"00031f466.jpg\")\n",
    "img = Image.open(path)\n",
    "\n",
    "img.format, img.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7204, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId_ClassId</th>\n",
       "      <th>EncodedPixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>004f40c73.jpg_1</td>\n",
       "      <td>1 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>004f40c73.jpg_2</td>\n",
       "      <td>1 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>004f40c73.jpg_3</td>\n",
       "      <td>1 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>004f40c73.jpg_4</td>\n",
       "      <td>1 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>006f39c41.jpg_1</td>\n",
       "      <td>1 1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ImageId_ClassId EncodedPixels\n",
       "0  004f40c73.jpg_1           1 1\n",
       "1  004f40c73.jpg_2           1 1\n",
       "2  004f40c73.jpg_3           1 1\n",
       "3  004f40c73.jpg_4           1 1\n",
       "4  006f39c41.jpg_1           1 1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df = pd.read_csv('../input/sample_submission.csv')\n",
    "print(submission_df.shape)\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['004f40c73.jpg', '006f39c41.jpg', '00b7fb703.jpg', ...,\n",
       "       'ffbf79783.jpg', 'ffc9a6187.jpg', 'ffdb60677.jpg'], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_test_images = submission_df['ImageId_ClassId'].apply(\n",
    "    lambda x: x.split('_')[0]\n",
    ").unique()\n",
    "\n",
    "unique_test_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "\n",
    "This EDA will mainly focus on detecting how the null masks are distributed. We will group all the `ImageId_ClassId` by their respective ImageId, and keep track of the number of missing masks for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId_ClassId</th>\n",
       "      <th>EncodedPixels</th>\n",
       "      <th>isNan</th>\n",
       "      <th>ImageId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0002cc93b.jpg_1</td>\n",
       "      <td>29102 12 29346 24 29602 24 29858 24 30114 24 3...</td>\n",
       "      <td>False</td>\n",
       "      <td>0002cc93b.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002cc93b.jpg_2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0002cc93b.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0002cc93b.jpg_3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0002cc93b.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0002cc93b.jpg_4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0002cc93b.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00031f466.jpg_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>00031f466.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ImageId_ClassId                                      EncodedPixels  isNan  \\\n",
       "0  0002cc93b.jpg_1  29102 12 29346 24 29602 24 29858 24 30114 24 3...  False   \n",
       "1  0002cc93b.jpg_2                                                NaN   True   \n",
       "2  0002cc93b.jpg_3                                                NaN   True   \n",
       "3  0002cc93b.jpg_4                                                NaN   True   \n",
       "4  00031f466.jpg_1                                                NaN   True   \n",
       "\n",
       "         ImageId  \n",
       "0  0002cc93b.jpg  \n",
       "1  0002cc93b.jpg  \n",
       "2  0002cc93b.jpg  \n",
       "3  0002cc93b.jpg  \n",
       "4  00031f466.jpg  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['isNan'] = pd.isna(train_df['EncodedPixels'])\n",
    "train_df['ImageId'] = train_df['ImageId_ClassId'].apply(\n",
    "    lambda x: x.split('_')[0]\n",
    ")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>missingCount</th>\n",
       "      <th>allMissing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0002cc93b.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00031f466.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000418bfc.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000789191.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0007a71bf.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ImageId  missingCount  allMissing\n",
       "0  0002cc93b.jpg             3           0\n",
       "1  00031f466.jpg             4           1\n",
       "2  000418bfc.jpg             4           1\n",
       "3  000789191.jpg             4           1\n",
       "4  0007a71bf.jpg             3           0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_nan_df = train_df.groupby(by='ImageId', axis=0).agg('sum')\n",
    "train_nan_df.reset_index(inplace=True)\n",
    "train_nan_df.rename(columns={'isNan': 'missingCount'}, inplace=True)\n",
    "train_nan_df['missingCount'] = train_nan_df['missingCount'].astype(np.int32)\n",
    "train_nan_df['allMissing'] = (train_nan_df['missingCount'] == 4).astype(int)\n",
    "\n",
    "train_nan_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1801, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>004f40c73.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>006f39c41.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00b7fb703.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00bbcd9af.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0108ce457.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ImageId\n",
       "0  004f40c73.jpg\n",
       "1  006f39c41.jpg\n",
       "2  00b7fb703.jpg\n",
       "3  00bbcd9af.jpg\n",
       "4  0108ce457.jpg"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_nan_df = pd.DataFrame(unique_test_images, columns=['ImageId'])\n",
    "print(test_nan_df.shape)\n",
    "test_nan_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    6239\n",
       "4    5902\n",
       "2     425\n",
       "1       2\n",
       "Name: missingCount, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_nan_df['missingCount'].hist()\n",
    "train_nan_df['missingCount'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that an overwhelming number of images have 3 to 4 missing masks. In fact, all except 2 images have 2+ missing masks. This means that we would basically need to train the final segmentation model on less than half of the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing Image Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_ = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(code, base, resize=True):\n",
    "    path = f'{base}/{code}'\n",
    "    img = cv2.imread(path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    if resize:\n",
    "        img = cv2.resize(img, (length_, 256))\n",
    "    return img\n",
    "\n",
    "def validate_path(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12568/12568 [07:15<00:00, 28.85it/s]\n"
     ]
    }
   ],
   "source": [
    "train_path = '../tmp/train'\n",
    "validate_path(train_path)\n",
    "\n",
    "for code in tqdm(train_nan_df['ImageId']):\n",
    "    img = load_img(\n",
    "        code,\n",
    "        base='../input/train_images',\n",
    "        resize=False\n",
    "    )\n",
    "    path = code.replace('.jpg', '')\n",
    "    cv2.imwrite(f'{train_path}/{path}.png', img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nan_df['ImageId'] = train_nan_df['ImageId'].apply(\n",
    "    lambda x: x.replace('.jpg', '.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow\n",
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10683 validated image filenames.\n",
      "Found 1885 validated image filenames.\n",
      "Found 1801 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "def create_datagen():\n",
    "    return ImageDataGenerator(\n",
    "        zoom_range=0.1,  # set range for random zoom\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='constant',\n",
    "        cval=0.,\n",
    "        rotation_range=10,\n",
    "        height_shift_range=0.1,\n",
    "        width_shift_range=0.1,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        rescale=1/255.,\n",
    "        validation_split=0.15\n",
    "    )\n",
    "\n",
    "def create_test_gen():\n",
    "    return ImageDataGenerator(rescale=1/255.).flow_from_dataframe(\n",
    "        test_nan_df,\n",
    "        directory='../input/test_images/',\n",
    "        x_col='ImageId',\n",
    "        class_mode=None,\n",
    "        target_size=(length_, 256),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "def create_flow(datagen, subset):\n",
    "    return datagen.flow_from_dataframe(\n",
    "        train_nan_df, \n",
    "#         directory='../input/train_images/',\n",
    "        directory='../tmp/train/',\n",
    "        x_col='ImageId', \n",
    "        y_col='allMissing', \n",
    "        class_mode='other',\n",
    "        target_size=(length_, 256),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        subset=subset\n",
    "    )\n",
    "\n",
    "# Using original generator\n",
    "data_generator = create_datagen()\n",
    "train_gen = create_flow(data_generator, 'training')\n",
    "val_gen = create_flow(data_generator, 'validation')\n",
    "test_gen = create_test_gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.densenet import DenseNet121\n",
    "\n",
    "def build_model():\n",
    "#     densenet = DenseNet121(\n",
    "#         include_top=False,\n",
    "#         input_shape=(256,256,3),\n",
    "#         weights='../input/densenet-keras/DenseNet-BC-121-32-no-top.h5'\n",
    "#     )\n",
    "    densenet = DenseNet121(include_top=False, weights='imagenet', input_shape=(length_, 256, 3))\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(densenet)\n",
    "    model.add(layers.GlobalAveragePooling2D())\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=Nadam(),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "densenet121 (Model)          (None, 25, 8, 1024)       7037504   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 7,568,961\n",
      "Trainable params: 7,482,241\n",
      "Non-trainable params: 86,720\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "  59/1335 [>.............................] - ETA: 1:52:02 - loss: 1.1778 - acc: 0.5572"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-ded1f33f847d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_steps\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_lr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    214\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_steps = train_nan_df.shape[0] / BATCH_SIZE\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'remove_model.h5', \n",
    "    monitor='val_acc', \n",
    "    verbose=1, \n",
    "    save_best_only=True, \n",
    "    save_weights_only=False,\n",
    "    mode='auto'\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "# training\n",
    "history = model.fit_generator(\n",
    "    train_gen,\n",
    "    steps_per_epoch=total_steps * 0.85,\n",
    "    validation_data=val_gen,\n",
    "    validation_steps=total_steps * 0.15,\n",
    "    epochs=40,\n",
    "    callbacks=[checkpoint, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history.history)\n",
    "history_df[['loss', 'val_loss']].plot()\n",
    "history_df[['acc', 'val_acc']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results as CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('../save_model/remove_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = model.predict_generator(\n",
    "    test_gen,\n",
    "    steps=len(test_gen),\n",
    "    verbose=1\n",
    ")\n",
    "test_nan_df['allMissing'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df.to_csv('history.csv', index=False)\n",
    "train_nan_df.to_csv('train_missing_count.csv', index=False)\n",
    "test_nan_df.to_csv('test_missing_count.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
